{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "from model import Model # tensorflow model define file\n",
    "import input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "INPUT_SIZE = 240\n",
    "DST_INPUT_SIZE = 128\n",
    "\n",
    "NUM_CLASS = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = 'log/data_%s/' % datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('trainDir', LOGDIR, 'DIrectory to put the training data')\n",
    "flags.DEFINE_string('tfrecordFileName', \"train_tf_file_%sx%s.tfrecords\" % (DST_INPUT_SIZE, DST_INPUT_SIZE), 'File name of train tfrecord')\n",
    "flags.DEFINE_integer('maxSteps', 200000, 'Number of steps to run trainer')\n",
    "flags.DEFINE_integer('batchSize', 90, 'train data size of subset')\n",
    "flags.DEFINE_integer('numExamplesPerEpochForTrain', 20000, 'input image num(include inflation)')\n",
    "flags.DEFINE_integer('threadNum', 4, 'num of threads')\n",
    "flags.DEFINE_float('learningLate', 1e-4, 'Initial learning rate')\n",
    "flags.DEFINE_string('f', '', 'kernel') # エラー回避"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(exampleProto):\n",
    "    features={\n",
    "            'label': tf.FixedLenFeature((), tf.int64, default_value=0),\n",
    "            'image': tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
    "        }\n",
    "    parsedFeatures = tf.parse_single_example(exampleProto, features)  # データ構造を解析\n",
    "     \n",
    "    return parsedFeatures[\"image\"], parsedFeatures[\"label\"]\n",
    " \n",
    "# データ解析（２）\n",
    "def read_image(argImages, argLabels):      \n",
    "    images = tf.decode_raw(argImages, tf.uint8)\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = tf.reshape(images, [DST_INPUT_SIZE, DST_INPUT_SIZE, 3])\n",
    "    \n",
    "    images = tf.image.resize_images(images, [IMAGE_SIZE, IMAGE_SIZE], method=tf.image.ResizeMethod.BICUBIC)\n",
    "    \n",
    "    ### 水増し\n",
    "    # 切り取り\n",
    "    cropsize = random.randint(INPUT_SIZE, INPUT_SIZE + (IMAGE_SIZE - INPUT_SIZE) / 2)\n",
    "    framesize = INPUT_SIZE + (cropsize - INPUT_SIZE) * 2\n",
    "    images = tf.image.resize_image_with_crop_or_pad(images, framesize, framesize)\n",
    "    images = tf.random_crop(images, [cropsize, cropsize, 3])\n",
    "    \n",
    "    # 回転\n",
    "    angle = random.randint(-15, 15)\n",
    "    radian = angle * math.pi / 180\n",
    "    images = tf.contrib.image.rotate(images, radian, interpolation=\"BILINEAR\")\n",
    "    \n",
    "    # 左右反転\n",
    "    images = tf.image.random_flip_left_right(images)\n",
    "    \n",
    "    # 輝度変化\n",
    "    images = tf.image.random_brightness(images, max_delta=0.2)\n",
    "    \n",
    "    # コントラスト変化\n",
    "    images = tf.image.random_contrast(images, lower=0.6, upper=1.4)\n",
    "    images = tf.image.random_hue(images, max_delta=0.04)\n",
    "    images = tf.image.random_saturation(images, lower=0.6, upper=1.4)\n",
    "    \n",
    "    # サイズもとに戻す\n",
    "    images = tf.image.resize_images(images, [DST_INPUT_SIZE, DST_INPUT_SIZE], method=tf.image.ResizeMethod.BICUBIC)\n",
    "    \n",
    "    images = images / 255\n",
    "    images = tf.reshape(images, [DST_INPUT_SIZE, DST_INPUT_SIZE, 3])\n",
    "    \n",
    "    labels = tf.cast(argLabels, tf.int32)\n",
    "    labels = tf.one_hot(labels, NUM_CLASS)\n",
    "     \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " with tf.Graph().as_default():\n",
    "        \n",
    "    threshold = 1e-10 # これ以下の誤差であれば終了\n",
    "    bestLossResult = 0.01\n",
    "    bestResult = []\n",
    "    bestStep = None\n",
    "    bestSteps = []\n",
    "    bestSession = None\n",
    "        \n",
    "    # 入力層\n",
    "    filenames = tf.placeholder(tf.string, shape=[None])\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(_parse_function, FLAGS.threadNum)  # レコードを解析し、テンソルに変換\n",
    "    dataset = dataset.map(read_image, FLAGS.threadNum)  # データの形式、形状を変更\n",
    "    dataset = dataset.shuffle(FLAGS.numExamplesPerEpochForTrain)\n",
    "    dataset = dataset.batch(FLAGS.batchSize)  # 連続するレコードをバッチに結合\n",
    "    dataset = dataset.repeat(-1)  # 無限に繰り返す\n",
    "    iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)  # イテレータを作成\n",
    "    images, labels = iterator.get_next()  # イテレータの次の要素を取得\n",
    "    tf.summary.image('image', images, max_outputs = 100)\n",
    "\n",
    "    initOp = iterator.make_initializer(dataset)  # イテレータを初期化\n",
    "        \n",
    "    model = Model(images, labels, DST_INPUT_SIZE, NUM_CLASS, FLAGS.batchSize, trainable=True)\n",
    "        \n",
    "    globalStep = tf.train.get_or_create_global_step()\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS.learningLate)\n",
    "    extraUpdataOps = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extraUpdataOps):\n",
    "        trainOp = optimizer.minimize(model.loss, global_step=globalStep)\n",
    "\n",
    "    tf.add_to_collection('train_op', trainOp)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        init = tf.global_variables_initializer()\n",
    "        session.run(init)  # 変数の初期化処理\n",
    "        session.run(initOp, feed_dict={filenames: [\"train_tf_file_%sx%s.tfrecords\" % (DST_INPUT_SIZE, DST_INPUT_SIZE)]})  # データの初期化\n",
    "        \n",
    "        saver = tf.train.Saver(max_to_keep = 0)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        summaryWriter = tf.summary.FileWriter(FLAGS.trainDir, session.graph)\n",
    "\n",
    "        for i in range(FLAGS.maxSteps):\n",
    "            startTime = time.time()\n",
    "            _, lossResult, accuracyResult = session.run([trainOp, model.loss, model.accuracy], feed_dict={model.keepProb: 0.75, model.isTraining: True})\n",
    "            step = session.run(globalStep)\n",
    "            duration = time.time() - startTime\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                numExamplesPerStep = FLAGS.batchSize\n",
    "                examplesPerSec = numExamplesPerStep / duration\n",
    "                secPerBatch = float(duration)\n",
    "                formatStr = ('%s: step %d, loss = %.5f (%.1f examples/sec; %.3f sec/batch)')\n",
    "                print(formatStr % (datetime.now(), step, lossResult, examplesPerSec, secPerBatch))\n",
    "                print('accuracy result', accuracyResult)\n",
    "\n",
    "            if step % 25 == 0 or step == 1:\n",
    "                summaryStr = session.run(summaryOp, feed_dict={model.keepProb: 1.0, model.isTraining: True})\n",
    "                summaryWriter.add_summary(summaryStr, step)\n",
    "\n",
    "            if step % 1000 == 0 or step == FLAGS.maxSteps or lossResult < threshold:\n",
    "                # 1000回ごとの最良を保存\n",
    "                if bestSession is not None:\n",
    "                    checkpointPath = os.path.join(FLAGS.trainDir, 'model.ckpt')\n",
    "                    saver.save(bestSession, checkpointPath, global_step = bestStep)\n",
    "                    bestResult.append(bestLossResult)\n",
    "                    bestSteps.append(bestStep)\n",
    "                    bestLossResult = 0.01\n",
    "                    bestStep = None\n",
    "                    bestSession = None\n",
    "                else:\n",
    "                    checkpointPath = os.path.join(FLAGS.trainDir, 'model.ckpt')\n",
    "                    saver.save(session, checkpointPath, global_step = step)\n",
    "\n",
    "            if lossResult < bestLossResult:\n",
    "                bestLossResult = lossResult\n",
    "                bestStep = step\n",
    "                bestSession = session\n",
    "\n",
    "            if lossResult < threshold:\n",
    "                print('loss is zero')\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
