{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.engine.network import Network\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数設定\n",
    "EXPAND_SIZE = 448\n",
    "CROP_SIZE = 400\n",
    "INPUT_SIZE = 224\n",
    "NUM_CLASS = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数設定\n",
    "LOGDIR = 'log/data_%s/' % datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('trainDir', LOGDIR, 'DIrectory to put the training data')\n",
    "flags.DEFINE_integer('maxEpoch', 100, 'Number of epochs to run trainer')\n",
    "flags.DEFINE_integer('batchSize', 64, 'train data size of subset')\n",
    "flags.DEFINE_integer('threadNum', 1, 'num of threads')\n",
    "flags.DEFINE_float('learningLate', 0.001, 'Initial learning rate')\n",
    "flags.DEFINE_string('f', '', 'kernel') # エラー回避"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TFRecordまわりの設定\n",
    "trainTFrecordFileName = \"train_tf_file_%sx%s.tfrecords\" % (INPUT_SIZE, INPUT_SIZE)\n",
    "testTFrecordFileName = \"test_tf_file_%sx%s.tfrecords\" % (INPUT_SIZE, INPUT_SIZE)\n",
    "numRecordsTrain = sum([1 for record  in tf.python_io.tf_record_iterator(trainTFrecordFileName)])\n",
    "numRecordsTest = sum([1 for record  in tf.python_io.tf_record_iterator(testTFrecordFileName)])\n",
    "stepsPerEpochTrain = (numRecordsTrain - 1) // FLAGS.batchSize + 1\n",
    "stepsPerEpochTest = (numRecordsTest - 1) // FLAGS.batchSize + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(exampleProto):\n",
    "    features={\n",
    "            'label': tf.FixedLenFeature((), tf.int64, default_value=0),\n",
    "            'image': tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
    "        }\n",
    "    parsedFeatures = tf.parse_single_example(exampleProto, features)  # データ構造を解析\n",
    "     \n",
    "    return parsedFeatures[\"image\"], parsedFeatures[\"label\"]\n",
    " \n",
    "def read_image_train(argImages, argLabels):      \n",
    "    images = tf.decode_raw(argImages, tf.uint8)\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = tf.reshape(images, [INPUT_SIZE, INPUT_SIZE, 3])\n",
    "    images = tf.image.resize_images(images, [EXPAND_SIZE, EXPAND_SIZE], method=tf.image.ResizeMethod.BICUBIC)\n",
    "    \n",
    "    ### 水増し\n",
    "    # 切り取り\n",
    "    cropsize = random.randint(CROP_SIZE, CROP_SIZE + (EXPAND_SIZE - CROP_SIZE) / 2)\n",
    "    framesize = CROP_SIZE + (cropsize - CROP_SIZE) * 2\n",
    "    images = tf.image.resize_image_with_crop_or_pad(images, framesize, framesize)\n",
    "    images = tf.random_crop(images, [cropsize, cropsize, 3])\n",
    "    \n",
    "    # 左右反転\n",
    "    images = tf.image.random_flip_left_right(images)\n",
    "    \n",
    "    # サイズもとに戻す\n",
    "    images = tf.image.resize_images(images, [INPUT_SIZE, INPUT_SIZE], method=tf.image.ResizeMethod.BICUBIC)\n",
    "    \n",
    "    images = images / 255\n",
    "    images = tf.reshape(images, [INPUT_SIZE, INPUT_SIZE, 3])\n",
    "    \n",
    "    labels = tf.cast(argLabels, tf.int32)\n",
    "    labels = tf.one_hot(labels, NUM_CLASS)\n",
    "     \n",
    "    return images, labels\n",
    "\n",
    "def read_image_test(argImages, argLabels):     \n",
    "    images = tf.decode_raw(argImages, tf.uint8)\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = images / 255  # 画像データを、0～1の範囲に変換する\n",
    "    images = tf.reshape(images, [INPUT_SIZE, INPUT_SIZE, 3])\n",
    "    \n",
    "    labels = tf.cast(argLabels, tf.int32)\n",
    "    labels = tf.one_hot(labels, NUM_CLASS)\n",
    "     \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習中に検証を実施するためのコールバック\n",
    "class EvaluateInputTensor(Callback):\n",
    "    def __init__(self, model, steps, metrics_prefix=\"val\", verbose=1):\n",
    "        super(EvaluateInputTensor, self).__init__()\n",
    "        self.val_model = model\n",
    "        self.num_steps = steps\n",
    "        self.verbose = verbose\n",
    "        self.metrics_prefix = metrics_prefix\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # 重みパラメータは学習用と評価用で共有されているので、\n",
    "        # そのまま評価用モデルで評価すればよい\n",
    "        results = self.val_model.evaluate(\n",
    "            steps=int(self.num_steps),\n",
    "            verbose=self.verbose)\n",
    "        # 評価結果を出力\n",
    "        if self.verbose >= 0:\n",
    "            metrics_str = \"\"\n",
    "            for result, name in zip(results, self.val_model.metrics_names):\n",
    "                metric_name = self.metrics_prefix + \"_\" + name\n",
    "                logs[metric_name] = result\n",
    "                if self.verbose > 0:\n",
    "                    metrics_str += metric_name + \": \" + str(result) + \" \"\n",
    "            metrics_str += \"\\n\"\n",
    "            print(metrics_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoardにtrain testのaccuracy と lossを書き込む\n",
    "class TrainValTensorBoard(keras.callbacks.TensorBoard):\n",
    "    def __init__(self, logDir, summaryWriter, summaryOp, **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_logDir = os.path.join(logDir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_logDir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_logDir = os.path.join(logDir, 'validation')\n",
    "        \n",
    "        self.writer = summaryWriter\n",
    "        self.op = summaryOp\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_logDir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "        \n",
    "        # tf.image 書き込み用\n",
    "        with tf.Session() as session:\n",
    "            self.writer.add_summary(session.run(self.op), epoch)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力層\n",
    "trainDataset = tf.data.TFRecordDataset(trainTFrecordFileName)\n",
    "trainDataset = trainDataset.map(_parse_function, FLAGS.threadNum)  # レコードを解析し、テンソルに変換\n",
    "trainDataset = trainDataset.map(read_image_train, FLAGS.threadNum)  # データの形式、形状を変更\n",
    "trainDataset = trainDataset.shuffle(numRecordsTrain)\n",
    "trainDataset = trainDataset.batch(FLAGS.batchSize)  # 連続するレコードをバッチに結合\n",
    "trainDataset = trainDataset.repeat(-1)  # 無限に繰り返す\n",
    "trainIterator = trainDataset.make_one_shot_iterator()\n",
    "trainImages, trainLabels = trainIterator.get_next()  # イテレータの次の要素を取得\n",
    "\n",
    "testDataset = tf.data.TFRecordDataset(testTFrecordFileName)\n",
    "testDataset = testDataset.map(_parse_function, FLAGS.threadNum)  # レコードを解析し、テンソルに変換\n",
    "testDataset = testDataset.map(read_image_test, FLAGS.threadNum)  # データの形式、形状を変更\n",
    "testDataset = testDataset.batch(FLAGS.batchSize)  # 連続するレコードをバッチに結合\n",
    "testDataset = testDataset.repeat(-1)  # 無限に繰り返す\n",
    "testIterator = testDataset.make_one_shot_iterator()\n",
    "testImages, testLabels = testIterator.get_next()   \n",
    "\n",
    "tf.summary.image('traing_image', trainImages, max_outputs = 100)\n",
    "\n",
    "summaryWriter = tf.summary.FileWriter(FLAGS.trainDir)\n",
    "summaryOp =  tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チェックポイントコールバックの指定\n",
    "checkPointPath = FLAGS.trainDir + \"weights_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.hdf5\"\n",
    "modelCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=checkPointPath,\n",
    "                    monitor='val_loss',\n",
    "                    verbose=1,\n",
    "                    save_best_only=True,\n",
    "                    mode='min',\n",
    "                    period=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputShape = (INPUT_SIZE, INPUT_SIZE, 3)\n",
    "inputTensor = Input(shape=inputShape)\n",
    "\n",
    "# モデル構築 Resnet50 face\n",
    "ResNet50Model = VGGFace(model='resnet50', weights='vggface', include_top=False, input_tensor=inputTensor, input_shape=inputShape)\n",
    "flat = GlobalAveragePooling2D()(ResNet50Model.output)\n",
    "dropout = Dropout(0.3)(flat)\n",
    "output = Dense(NUM_CLASS, activation='softmax')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重みを共有するネットワークの作成\n",
    "commonNetwork = Network(inputTensor, output)\n",
    "\n",
    "# train test それぞれのinput\n",
    "trainInputLayer = Input(tensor=trainImages)\n",
    "testInputLayer = Input(tensor=testImages)\n",
    "\n",
    "# output\n",
    "trainOutput = commonNetwork(trainInputLayer)\n",
    "testOutput = commonNetwork(testInputLayer)\n",
    "\n",
    "# train test モデル\n",
    "trainModel = Model(inputs=trainInputLayer, outputs=trainOutput)\n",
    "testModel = Model(inputs=testInputLayer, outputs=testOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定した層以降は学習する\n",
    "trainStartLayerName = 'activation_40'\n",
    "\n",
    "setTrainable = False\n",
    "for layer in trainModel.get_layer(\"network_1\").layers:\n",
    "    if layer.name == trainStartLayerName:\n",
    "        setTrainable = True\n",
    "    layer.trainable = setTrainable\n",
    "\n",
    "# layerの重みを学習するか確認のためのprint\n",
    "for index, layer in enumerate(trainModel.get_layer(\"network_1\").layers):\n",
    "    print(index, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンパイル optimizerは好み？\n",
    "trainModel.compile(optimizer='nadam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'],\n",
    "                   target_tensors=[trainLabels])\n",
    "testModel.compile(optimizer='nadam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'],\n",
    "                   target_tensors=[testLabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochHistory = trainModel.fit(\n",
    "                epochs=FLAGS.maxEpoch, \n",
    "                verbose=1,\n",
    "                steps_per_epoch=stepsPerEpochTrain,\n",
    "                callbacks=[\n",
    "                EvaluateInputTensor(testModel, steps=stepsPerEpochTest),\n",
    "                modelCheckPoint,\n",
    "                TrainValTensorBoard(logDir=FLAGS.trainDir, summaryWriter=summaryWriter, summaryOp=summaryOp,\n",
    "                                    write_graph=True, write_images=True)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
