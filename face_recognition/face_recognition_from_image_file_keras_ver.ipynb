{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.engine.network import Network\n",
    "from keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, Input, MaxPooling2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append('../face_detection/')\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils_color as vis_util\n",
    "from tensorflow_face_detector import TensoflowFaceDector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 224\n",
    "NUM_CLASS = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    0: \"井口眞緒\",\n",
    "    1: \"潮紗理菜\",\n",
    "    2: \"柿崎芽実\",\n",
    "    3: \"影山優佳\",\n",
    "    4: \"加藤史帆\",\n",
    "    5: \"齊藤京子\",\n",
    "    6: \"佐々木久美\",\n",
    "    7: \"佐々木美玲\",\n",
    "    8: \"高瀬愛奈\" ,\n",
    "    9: \"高本彩花\" ,\n",
    "    10: \"東村芽依\",\n",
    "    11: \"金村美玖\",\n",
    "    12: \"河田陽菜\",\n",
    "    13: \"小坂菜緒\",\n",
    "    14: \"富田鈴花\",\n",
    "    15: \"丹生明里\",\n",
    "    16: \"濱岸ひより\",\n",
    "    17: \"松田好花\",\n",
    "    18: \"宮田愛萌\",\n",
    "    19: \"渡邉美穂\",\n",
    "    20: \"上村ひなの\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ解析\n",
    "def read_image(images):     \n",
    "    image = tf.cast(images, tf.float32)\n",
    "    image = image / 255  # 画像データを、0～1の範囲に変換する\n",
    "    image = tf.reshape(image, [INPUT_SIZE, INPUT_SIZE, 3])\n",
    "     \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_n_faces(imageFile):\n",
    "    image = cv2.imread(imageFile)\n",
    "    faceDetectImage = image.copy()\n",
    "    [h, w] = faceDetectImage.shape[:2]\n",
    "\n",
    "    (boxes, scores, classes, num_detections) = tDetector.run(faceDetectImage)\n",
    "\n",
    "    faceBoxes = vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        faceDetectImage,\n",
    "        np.squeeze(boxes),\n",
    "        np.squeeze(classes).astype(np.int32),\n",
    "        np.squeeze(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=4)\n",
    "    \n",
    "    personCount = len(faceBoxes)\n",
    "        \n",
    "    imageHeight, imageWidth = image.shape[:2]\n",
    "    \n",
    "    originBoxes = []\n",
    "    cropBoxes = []\n",
    "    \n",
    "    for index, box in enumerate(faceBoxes):\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        (left, right, top, bottom) = (int(xmin * imageWidth), int(xmax * imageWidth), int(ymin * imageHeight), int(ymax * imageHeight))\n",
    "        originBoxes.append((left, right, top, bottom))  # TensorFlowの顔検出で切り取った領域\n",
    "        \n",
    "        cropWidth = right - left\n",
    "        cropHeight = bottom - top\n",
    "        \n",
    "        # 長辺に合わせる\n",
    "        if cropHeight > cropWidth:\n",
    "            diff  = (cropHeight - cropWidth) / 2\n",
    "            if int(left - diff) < 0 or int(right + diff) > imageWidth:\n",
    "                top = int(top + diff)\n",
    "                bottom = int(bottom - diff)\n",
    "            else:\n",
    "                left = int(left - diff)\n",
    "                right = int(right + diff)\n",
    "        else:\n",
    "            diff = (cropWidth - cropHeight) / 2\n",
    "            if int(top - diff) < 0 or int(bottom + diff) > imageHeight:\n",
    "                left = int(left + diff)\n",
    "                right = int(right - diff)\n",
    "            else:\n",
    "                top = int(top - diff)\n",
    "                bottom = int(bottom + diff)\n",
    "        \n",
    "        cropBoxes.append((left, right, top, bottom))   # 顔検出に用いる正方形領域\n",
    "        \n",
    "    return originBoxes, cropBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropImage(imageFile):   \n",
    "    sourceImage = cv2.imread(imageFile)\n",
    "    boxedImage = sourceImage.copy() \n",
    "    \n",
    "    originBoxes, cropBoxes = detect_n_faces(imageFile)\n",
    "    \n",
    "    if len(cropBoxes) == 0:\n",
    "        return\n",
    "\n",
    "    cropImages = []\n",
    "    \n",
    "    for box in cropBoxes:\n",
    "        (left, right, top, bottom) = box\n",
    "        cropImages.append(sourceImage[top:bottom, left:right])\n",
    "        cv2.rectangle(boxedImage, (left, top), (right, bottom), (255, 0, 0), thickness=3)\n",
    "\n",
    "    imgs = np.empty((0, INPUT_SIZE, INPUT_SIZE, 3)) #empty dummy array, we will append to this array all the images\n",
    "    for img in cropImages:\n",
    "        img = cv2.resize(img, dsize=(INPUT_SIZE, INPUT_SIZE), interpolation=cv2.INTER_LANCZOS4)\n",
    "        img = img[:, :, ::-1].copy()\n",
    "        imgs = np.append(imgs, np.array(img).reshape((1, INPUT_SIZE, INPUT_SIZE, 3)), axis=0)\n",
    "        \n",
    "    imgs = imgs / 255 # 正規化(しないと正しくなくなる)\n",
    "\n",
    "    return boxedImage, cropImages, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://kazuhito00.hatenablog.com/entry/2018/06/20/025715\n",
    "from PIL import ImageFont, ImageDraw\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "class CvPutJaText:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def puttext(cls, cv_image, text, point, font_path, font_size, color=(0,0,0)):\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "        \n",
    "        cv_rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = PILImage.fromarray(cv_rgb_image)\n",
    "        \n",
    "        draw = ImageDraw.Draw(pil_image)\n",
    "        draw.text(point, text, fill=color, font=font)\n",
    "        \n",
    "        cv_rgb_result_image = np.asarray(pil_image)\n",
    "        cv_bgr_result_image = cv2.cvtColor(cv_rgb_result_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        return cv_bgr_result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "def display_cv_image(image, format='.jpg'):\n",
    "    decoded_bytes = cv2.imencode(format, image)[1].tobytes()\n",
    "    display(Image(data=decoded_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = '../face_detection/model/frozen_inference_graph_face.pb'\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = '../face_detection/protos/face_label_map.pbtxt'\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "tDetector = TensoflowFaceDector(PATH_TO_CKPT)# モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputShape = (INPUT_SIZE, INPUT_SIZE, 3)\n",
    "inputTensor = Input(shape=inputShape)\n",
    "\n",
    "# モデル構築 Resnet50 face\n",
    "ResNet50Model = VGGFace(model='resnet50', weights='vggface', include_top=False, input_tensor=inputTensor, input_shape=inputShape)\n",
    "flat = GlobalAveragePooling2D()(ResNet50Model.output)\n",
    "dropout = Dropout(0)(flat)\n",
    "output = Dense(NUM_CLASS, activation='softmax')(dropout)\n",
    "\n",
    "commonNetwork = Network(inputTensor, output)\n",
    "\n",
    "output = commonNetwork(inputTensor)\n",
    "model = Model(inputs=inputTensor, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggface ResNet50\n",
    "trainStartLayerName = 'activation_40'\n",
    "\n",
    "setTrainable = False\n",
    "for layer in model.get_layer(\"network_1\").layers:\n",
    "    if layer.name == trainStartLayerName:\n",
    "        setTrainable = True\n",
    "    layer.trainable = setTrainable\n",
    "    \n",
    "weightPath = 'recognition_model/weights.hdf5'\n",
    "model.load_weights(weightPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imageFIle = \"images/music_station.jpg\"\n",
    "boxedImage, cropedImages, imgs = cropImage(imageFIle)\n",
    "\n",
    "estimationResult = model.predict(x=imgs, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真っ白な画像をつくる\n",
    "startY = 12\n",
    "startX = 36\n",
    "DISP_SIZE = 128\n",
    "heightDivide = 210\n",
    "widthDivide = 200\n",
    "fontSize = 15\n",
    "\n",
    "height = int(heightDivide * np.ceil(len(cropedImages) / 5))\n",
    "width = 1000\n",
    "resultImage = np.zeros((height, width, 3), np.uint8)\n",
    "\n",
    "for h in range(0, height):\n",
    "    for w in range(0, width):\n",
    "        resultImage[h, w] = [255, 255, 255]\n",
    "        \n",
    "# 切り取った画像と識別結果を描画\n",
    "fontPath = './font/ipaexg.ttf'\n",
    "\n",
    "for i in range (0, len(cropedImages)):\n",
    "    q, mod = divmod(i, 5)\n",
    "    resizeImage = cv2.resize(cropedImages[i], dsize=(128, 128), interpolation=cv2.INTER_LANCZOS4)\n",
    "    imgX = startX + widthDivide * mod\n",
    "    imgY = startY + heightDivide * q\n",
    "    resultImage[imgY : imgY + DISP_SIZE, imgX : imgX + DISP_SIZE] = resizeImage\n",
    "    \n",
    "    # https://gist.github.com/naoyashiga/8f8a215932e881a3f9ec85e45d499e99\n",
    "    # 上位K件のインデックス\n",
    "    K = 3\n",
    "    unsortedMaxIndices = np.argpartition(-estimationResult[i], K)[:K]\n",
    "    \n",
    "    topK = estimationResult[i][unsortedMaxIndices]\n",
    "    indices = np.argsort(-topK)\n",
    "    \n",
    "    maxKIndices = unsortedMaxIndices[indices]\n",
    "    \n",
    "    for index, m in enumerate(maxKIndices):\n",
    "        str = names[m] + \": \" + '{:.4f}'.format(estimationResult[i][m])\n",
    "        resultImage = CvPutJaText.puttext(resultImage, str,\n",
    "                                        (imgX, imgY + DISP_SIZE + 10 + fontSize * index), fontPath, fontSize, (0, 0, 0))\n",
    "        \n",
    "\n",
    "display_cv_image(resultImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_cv_image(boxedImage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
