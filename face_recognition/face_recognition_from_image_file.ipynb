{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append('../')\n",
    "from model import Model\n",
    "\n",
    "sys.path.append('../face_detection/')\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils_color as vis_util\n",
    "from tensorflow_face_detector import TensoflowFaceDector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    0: \"井口眞緒\",\n",
    "    1: \"潮紗理菜\",\n",
    "    2: \"柿崎芽実\",\n",
    "    3: \"影山優佳\",\n",
    "    4: \"加藤史帆\",\n",
    "    5: \"齊藤京子\",\n",
    "    6: \"佐々木久美\",\n",
    "    7: \"佐々木美玲\",\n",
    "    8: \"高瀬愛奈\" ,\n",
    "    9: \"高本彩花\" ,\n",
    "    10: \"東村芽依\",\n",
    "    11: \"金村美玖\",\n",
    "    12: \"河田陽菜\",\n",
    "    13: \"小坂菜緒\",\n",
    "    14: \"富田鈴花\",\n",
    "    15: \"丹生明里\",\n",
    "    16: \"濱岸ひより\",\n",
    "    17: \"松田好花\",\n",
    "    18: \"宮田愛萌\",\n",
    "    19: \"渡邉美穂\",\n",
    "    20: \"上村ひなの\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_n_faces(imageFile):\n",
    "    image = cv2.imread(imageFile)\n",
    "    faceDetectImage = image.copy()\n",
    "    [h, w] = faceDetectImage.shape[:2]\n",
    "\n",
    "    (boxes, scores, classes, num_detections) = tDetector.run(faceDetectImage)\n",
    "\n",
    "    faceBoxes = vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        faceDetectImage,\n",
    "        np.squeeze(boxes),\n",
    "        np.squeeze(classes).astype(np.int32),\n",
    "        np.squeeze(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=4)\n",
    "    \n",
    "    personCount = len(faceBoxes)\n",
    "        \n",
    "    imageHeight, imageWidth = image.shape[:2]\n",
    "    \n",
    "    originBoxes = []\n",
    "    cropBoxes = []\n",
    "    \n",
    "    for index, box in enumerate(faceBoxes):\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        (left, right, top, bottom) = (int(xmin * imageWidth), int(xmax * imageWidth), int(ymin * imageHeight), int(ymax * imageHeight))\n",
    "        originBoxes.append((left, right, top, bottom))  # TensorFlowの顔検出で切り取った領域\n",
    "        \n",
    "        cropWidth = right - left\n",
    "        cropHeight = bottom - top\n",
    "        \n",
    "        # 長辺に合わせる\n",
    "        if cropHeight > cropWidth:\n",
    "            diff  = (cropHeight - cropWidth) / 2\n",
    "            if int(left - diff) < 0 or int(right + diff) > imageWidth:\n",
    "                top = int(top + diff)\n",
    "                bottom = int(bottom - diff)\n",
    "            else:\n",
    "                left = int(left - diff)\n",
    "                right = int(right + diff)\n",
    "        else:\n",
    "            diff = (cropWidth - cropHeight) / 2\n",
    "            if int(top - diff) < 0 or int(bottom + diff) > imageHeight:\n",
    "                left = int(left + diff)\n",
    "                right = int(right - diff)\n",
    "            else:\n",
    "                top = int(top - diff)\n",
    "                bottom = int(bottom + diff)\n",
    "        \n",
    "        cropBoxes.append((left, right, top, bottom))   # 顔検出に用いる正方形領域\n",
    "        \n",
    "    return originBoxes, cropBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ解析\n",
    "def read_image(images):     \n",
    "    image = tf.cast(images, tf.float32)\n",
    "    image = image / 255  # 画像データを、0～1の範囲に変換する\n",
    "    image = tf.reshape(image, [128, 128, 3])\n",
    "     \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceIdentification(imageFile):\n",
    "    imageSize = 128\n",
    "    numClass = 21\n",
    "    \n",
    "    sourceImage = cv2.imread(imageFile)\n",
    "    boxedImage = sourceImage.copy() \n",
    "    \n",
    "    originBoxes, cropBoxes = detect_n_faces(imageFile)\n",
    "    \n",
    "    if len(cropBoxes) == 0:\n",
    "        return\n",
    "\n",
    "    cropImages = []\n",
    "    \n",
    "    for box in cropBoxes:\n",
    "        (left, right, top, bottom) = box\n",
    "        cropImages.append(sourceImage[top:bottom, left:right])\n",
    "        cv2.rectangle(boxedImage, (left, top), (right, bottom), (255, 0, 0), thickness=3)\n",
    "\n",
    "    imgs = np.empty((0, imageSize, imageSize, 3)) #empty dummy array, we will append to this array all the images\n",
    "    for img in cropImages:\n",
    "        img = cv2.resize(img, dsize=(imageSize, imageSize), interpolation=cv2.INTER_LANCZOS4)\n",
    "        img = img[:, :, ::-1].copy()\n",
    "        imgs = np.append(imgs, np.array(img).reshape((1, imageSize, imageSize, 3)), axis=0)\n",
    "        \n",
    "    with tf.Graph().as_default():\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(imgs)\n",
    "        dataset = dataset.map(read_image, len(cropImages))  # データの形式、形状を変更\n",
    "        dataset = dataset.batch(len(cropImages))  # 連続するレコードをバッチに結合\n",
    "        iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)  # イテレータを作成\n",
    "        images = iterator.get_next()  # イテレータの次の要素を取得\n",
    "\n",
    "        initOp = iterator.make_initializer(dataset)  # イテレータを初期化\n",
    "\n",
    "        model = Model(images, imageSize, numClass, len(cropImages), trainable=False)\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(initOp)  # データの初期化\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(session, ckptPath)\n",
    "\n",
    "            estimationResult, resultImages = session.run([model.logits, images], feed_dict={model.keepProb: 1.0, model.isTraining: False})\n",
    "\n",
    "    return boxedImage, cropImages, estimationResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://kazuhito00.hatenablog.com/entry/2018/06/20/025715\n",
    "from PIL import ImageFont, ImageDraw\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "class CvPutJaText:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def puttext(cls, cv_image, text, point, font_path, font_size, color=(0,0,0)):\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "        \n",
    "        cv_rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = PILImage.fromarray(cv_rgb_image)\n",
    "        \n",
    "        draw = ImageDraw.Draw(pil_image)\n",
    "        draw.text(point, text, fill=color, font=font)\n",
    "        \n",
    "        cv_rgb_result_image = np.asarray(pil_image)\n",
    "        cv_bgr_result_image = cv2.cvtColor(cv_rgb_result_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        return cv_bgr_result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "def display_cv_image(image, format='.jpg'):\n",
    "    decoded_bytes = cv2.imencode(format, image)[1].tobytes()\n",
    "    display(Image(data=decoded_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = '../face_detection/model/frozen_inference_graph_face.pb'\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = '../face_detection/protos/face_label_map.pbtxt'\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "tDetector = TensoflowFaceDector(PATH_TO_CKPT)# モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFIle = \"images/music_station.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckptPath = 'recognition_model/model.ckpt-101881'\n",
    "boxedImage, cropedImages, estimationResult = faceIdentification(imageFIle)\n",
    "\n",
    "# 真っ白な画像をつくる\n",
    "startY = 12\n",
    "startX = 36\n",
    "imageSize = 128\n",
    "heightDivide = 210\n",
    "widthDivide = 200\n",
    "fontSize = 15\n",
    "\n",
    "height = int(heightDivide * np.ceil(len(cropedImages) / 5))\n",
    "width = 1000\n",
    "resultImage = np.zeros((height, width, 3), np.uint8)\n",
    "\n",
    "for h in range(0, height):\n",
    "    for w in range(0, width):\n",
    "        resultImage[h, w] = [255, 255, 255]\n",
    "        \n",
    "# 切り取った画像と識別結果を描画\n",
    "fontPath = './font/ipaexg.ttf'\n",
    "\n",
    "for i in range (0, len(cropedImages)):\n",
    "    q, mod = divmod(i, 5)\n",
    "    resizeImage = cv2.resize(cropedImages[i], dsize=(128, 128), interpolation=cv2.INTER_LANCZOS4)\n",
    "    imgX = startX + widthDivide * mod\n",
    "    imgY = startY + heightDivide * q\n",
    "    resultImage[imgY : imgY + imageSize, imgX : imgX + imageSize] = resizeImage\n",
    "    \n",
    "    # https://gist.github.com/naoyashiga/8f8a215932e881a3f9ec85e45d499e99\n",
    "    # 上位K件のインデックス\n",
    "    K = 3\n",
    "    unsortedMaxIndices = np.argpartition(-estimationResult[i], K)[:K]\n",
    "    \n",
    "    topK = estimationResult[i][unsortedMaxIndices]\n",
    "    indices = np.argsort(-topK)\n",
    "    \n",
    "    maxKIndices = unsortedMaxIndices[indices]\n",
    "    \n",
    "    for index, m in enumerate(maxKIndices):\n",
    "        str = names[m] + \": \" + '{:.4f}'.format(estimationResult[i][m])\n",
    "        resultImage = CvPutJaText.puttext(resultImage, str,\n",
    "                                        (imgX, imgY + imageSize + 10 + fontSize * index), fontPath, fontSize, (0, 0, 0))\n",
    "        \n",
    "\n",
    "display_cv_image(resultImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_cv_image(boxedImage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
